{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # BE QUIET!!!! (info and warnings are not printed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" for 2080 Ti\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saidinesh/.conda/envs/tensorflow-test/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() #compatibility\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "# from tools import problems,networks,train\n",
    "import math\n",
    "import sys\n",
    "import numpy.linalg as la\n",
    "\n",
    "# from sparc_sim import sparc_sim\n",
    "# from sparc_se import sparc_se\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncode_params   = {'P': 15.0,    # Average codeword symbol power constraint\\n                 'R': 1.3,     # Rate\\n                 'L': 100,    # Number of sections\\n                 'M': 32}      # Columns per section\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "M = Number of columns in a section \n",
    "L = Number of section\n",
    "R = Rate\n",
    "pnz = sparsity\n",
    "awgn_var = AWGN channel variance\n",
    "\"\"\"\n",
    "\n",
    "P, M, L, R, pnz, awgn_var = 15, 32, 50, 0.5, 0.1, 0.01       #try with L=1000 sections later, changed R from 1.3 to 1.2\n",
    "K = 1    # for unmodulated case\n",
    "\n",
    "'''\n",
    "code_params   = {'P': 15.0,    # Average codeword symbol power constraint\n",
    "                 'R': 1.3,     # Rate\n",
    "                 'L': 100,    # Number of sections\n",
    "                 'M': 32}      # Columns per section\n",
    "'''                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Message vector from bit stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum value for no_cols is 2 and should be even\n",
    "rng = np.random.RandomState(seed=None) \n",
    "\n",
    "def gen_msg(no_cols,M, L, K, R, rng):\n",
    "    #no_cols = 2000\n",
    "    colsby2 = int(np.divide(no_cols,2))\n",
    "\n",
    "    beta1 = np.zeros((L*M,colsby2))\n",
    "    beta_val = np.zeros((L*M, colsby2) )\n",
    "\n",
    "    bit_len = int(round(L*np.log2(K*M)))\n",
    "    logM = int(round(np.log2(M)))\n",
    "    sec_size = logM\n",
    "    L = bit_len // sec_size\n",
    "\n",
    "    W = np.array(P)\n",
    "\n",
    "    for i in range(1,no_cols):\n",
    "        #bit_len = int(round(L*np.log2(K*M)))   \n",
    "        bits_in = rng.randint(2, size=bit_len)  #generates a boolean vector of length 5000  (removed dtype=' bool')\n",
    "\n",
    "        beta0 = np.zeros(L*M)\n",
    "        for l in range(L):\n",
    "            bits_sec = bits_in[l*sec_size : l*sec_size + logM]\n",
    "            assert 0 < logM < 64\n",
    "            idx = bits_sec.dot(1 << np.arange(logM)[::-1])  # idx = decimal equivalent of the 5 bits (eg: 10100 will give 20)\n",
    "            beta0[l*M + idx] = 1      # will make a 1 at the decimal equivalent in the l-th section\n",
    "\n",
    "    # beta0 is a M*L length message vector that has 1 at decimal values of (5 bit stream) 32 length section.   \n",
    "\n",
    "        n = int(round(bit_len/R))\n",
    "        R_actual = bit_len / n      # Actual rate\n",
    "\n",
    "        #beta0 = beta0.reshape((L*M),1)\n",
    "        if i <= colsby2:\n",
    "            beta1[:,i-1] = beta0\n",
    "        else:\n",
    "            beta_val[:,i - no_cols] = beta0    \n",
    "\n",
    "    return beta1, beta_val, n, colsby2       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1, beta_val, n, colsby2 = gen_msg(2000,M, L, K, R, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "print(beta1[:,1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Measurement Matrix\n",
    "Right now, we are working with Random i.i.d gaussian matrices with N(0,1/n). Later we may use FFT/DCT matrices for better computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(size=(n, L*M), scale=1.0 / math.sqrt(n)).astype(np.float32)\n",
    "A_ = tf.constant(A,name='A',dtype = tf.float32 )  #Made a tf constant since it need not be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta1 is for training, beta_val is for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgen_ = tf.constant(beta1, dtype= tf.float32)  #changed to tf.constant\n",
    "#xgen_ = tf.cast(beta1, dtype= tf.float32)\n",
    "\n",
    "ygen_ = tf.matmul(A_,xgen_) + tf.random.normal( (n,colsby2),stddev=math.sqrt( awgn_var ) )\n",
    "y_val = np.matmul(A, beta_val) + np.random.normal( size = (n,colsby2),scale= math.sqrt(awgn_var)) #tf.sqrt(awgn_var)*rng.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgen_, ygen_ is given for training\n",
    "beta_val, y_val is for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1600, 1000), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(500, 1000), dtype=float32)\n",
      "Tensor(\"A:0\", shape=(500, 1600), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(xgen_)\n",
    "print(ygen_)\n",
    "print(A_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD LISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, initial_lambda = 6, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_soft_threshold(r_, lam_):\n",
    "    \"implement a soft threshold function y=sign(r)*max(0,abs(r)-lam)\"\n",
    "    lam_ = tf.maximum(lam_, 0)\n",
    "    return tf.sign(r_) * tf.maximum(tf.abs(r_) - lam_, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating placeholders, which will hold dataset later.\n",
    "x_ = tf.compat.v1.placeholder( tf.float32,(L*M,None),name='x' )\n",
    "y_ = tf.compat.v1.placeholder( tf.float32,(n,None),name='y' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = simple_soft_threshold #creating an object\n",
    "B = A.T / (1.01 * la.norm(A,2)**2) #(This is according to original code, not sure why)\n",
    "#B = A.T #numpy array\n",
    "\n",
    "# All of these below are tf, which means they will be stored as graphs and will run later. \n",
    "# B and S are the trainable parameters\n",
    "B_ = tf.Variable(B,dtype=tf.float32,name='B_0') #creating tf.variable to make it a trainable parameter. \n",
    "S_ = tf.Variable(np.identity(L*M) - np.matmul(B, A),dtype=tf.float32,name='S_0' )\n",
    "\n",
    "By_ = tf.matmul(B_,y_)  # This will be the input for the shrinkage function in the first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'B_0:0' shape=(1600, 500) dtype=float32_ref>\n",
      "Tensor(\"MatMul_1:0\", shape=(1600, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(B_)\n",
    "print(By_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why it is written as a list. Might get clear later.\n",
    "layers = []\n",
    "layers.append( ('Linear',By_,None) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lambda = np.array(initial_lambda).astype(np.float32)\n",
    "lam0_ = tf.Variable(initial_lambda,name='lam_0')  #to make it learnable\n",
    "\n",
    "xhat_ = eta( By_, lam0_) #first itertion and xhat_0 is all 0s\n",
    "layers.append( ('LISTA T=1',xhat_, (lam0_,) ) )\n",
    "\n",
    "for t in range(1,T):\n",
    "    lam_ = tf.Variable( initial_lambda,name='lam_{0}'.format(t) ) # using the same lambda\n",
    "    xhat_ = eta( tf.matmul(S_,xhat_) + By_, lam_ )  # will be stored as graphs which will run during a session later.   \n",
    "    layers.append( ('LISTA T='+str(t+1),xhat_,(lam_,)) ) # creating layers (xhat_ is an operation)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Linear', <tf.Tensor 'MatMul_1:0' shape=(1600, ?) dtype=float32>, None)\n",
      "('LISTA T=1', <tf.Tensor 'mul:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_0:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=2', <tf.Tensor 'mul_1:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_1:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=3', <tf.Tensor 'mul_2:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_2:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=4', <tf.Tensor 'mul_3:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_3:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=5', <tf.Tensor 'mul_4:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_4:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=6', <tf.Tensor 'mul_5:0' shape=(1600, ?) dtype=float32>, (<tf.Variable 'lam_5:0' shape=() dtype=float32_ref>,))\n",
      "<tf.Variable 'lam_0:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(*layers , sep = \"\\n\")\n",
    "print(lam0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul_2:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul_3:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul_4:0\", shape=(1600, ?), dtype=float32)\n",
      "Tensor(\"mul_5:0\", shape=(1600, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for name,xhat_,var_list in layers:\n",
    "    print(xhat_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training LISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trinit,refinements,final_refine = 1e-3, (.5,.1,.01), None\n",
    "losses_=[]\n",
    "nmse_=[]\n",
    "trainers_=[]\n",
    "\n",
    "\"\"\"assert is used when, If the statement is true then it does nothing and continues \n",
    "the execution, but if the statement is False then it stops the execution of the program \n",
    "and throws an error.\n",
    "\"\"\"\n",
    "assert np.array(refinements).min()>0,'all refinements must be in (0,1]'\n",
    "assert np.array(refinements).max()<=1,'all refinements must be in (0,1]'\n",
    "\n",
    "tr_ = tf.Variable(trinit,name='tr',trainable=False) #Learning rate\n",
    "training_stages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior() #compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_denom_ = tf.nn.l2_loss(x_)  #computes half of l2 loss without the sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,xhat_,var_list in layers:\n",
    "    loss_  = tf.nn.l2_loss( xhat_ - x_) #original values of x will be stored in the placeholder\n",
    "    nmse_  = tf.nn.l2_loss( (xhat_ - x_) ) / nmse_denom_ # computing the nmse.\n",
    "    if var_list is not None:\n",
    "            train_ = tf.train.AdamOptimizer(tr_).minimize(loss_, var_list=var_list) \n",
    "            training_stages.append( (name,xhat_,loss_,nmse_,train_,var_list) )\n",
    "    for fm in refinements:\n",
    "            train2_ = tf.train.AdamOptimizer(tr_*fm).minimize(loss_)\n",
    "            training_stages.append( (name+' trainrate=' + str(fm) ,xhat_,loss_,nmse_,train2_,()) )\n",
    "            \n",
    "if final_refine:\n",
    "    train2_ = tf.train.AdamOptimizer(tr_*final_refine).minimize(loss_)\n",
    "    training_stages.append( (name+' final refine ' + str(final_refine) ,xhat_,loss_,nmse_,train2_,()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Linear trainrate=0.5', <tf.Tensor 'MatMul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_1:0' shape=() dtype=float32>, <tf.Tensor 'truediv:0' shape=() dtype=float32>, <tf.Operation 'Adam' type=NoOp>, ())\n",
      "('Linear trainrate=0.1', <tf.Tensor 'MatMul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_1:0' shape=() dtype=float32>, <tf.Tensor 'truediv:0' shape=() dtype=float32>, <tf.Operation 'Adam_1' type=NoOp>, ())\n",
      "('Linear trainrate=0.01', <tf.Tensor 'MatMul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_1:0' shape=() dtype=float32>, <tf.Tensor 'truediv:0' shape=() dtype=float32>, <tf.Operation 'Adam_2' type=NoOp>, ())\n",
      "('LISTA T=1', <tf.Tensor 'mul:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_3:0' shape=() dtype=float32>, <tf.Tensor 'truediv_1:0' shape=() dtype=float32>, <tf.Operation 'Adam_3' type=NoOp>, (<tf.Variable 'lam_0:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=1 trainrate=0.5', <tf.Tensor 'mul:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_3:0' shape=() dtype=float32>, <tf.Tensor 'truediv_1:0' shape=() dtype=float32>, <tf.Operation 'Adam_4' type=NoOp>, ())\n",
      "('LISTA T=1 trainrate=0.1', <tf.Tensor 'mul:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_3:0' shape=() dtype=float32>, <tf.Tensor 'truediv_1:0' shape=() dtype=float32>, <tf.Operation 'Adam_5' type=NoOp>, ())\n",
      "('LISTA T=1 trainrate=0.01', <tf.Tensor 'mul:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_3:0' shape=() dtype=float32>, <tf.Tensor 'truediv_1:0' shape=() dtype=float32>, <tf.Operation 'Adam_6' type=NoOp>, ())\n",
      "('LISTA T=2', <tf.Tensor 'mul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_2:0' shape=() dtype=float32>, <tf.Operation 'Adam_7' type=NoOp>, (<tf.Variable 'lam_1:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=2 trainrate=0.5', <tf.Tensor 'mul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_2:0' shape=() dtype=float32>, <tf.Operation 'Adam_8' type=NoOp>, ())\n",
      "('LISTA T=2 trainrate=0.1', <tf.Tensor 'mul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_2:0' shape=() dtype=float32>, <tf.Operation 'Adam_9' type=NoOp>, ())\n",
      "('LISTA T=2 trainrate=0.01', <tf.Tensor 'mul_1:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_2:0' shape=() dtype=float32>, <tf.Operation 'Adam_10' type=NoOp>, ())\n",
      "('LISTA T=3', <tf.Tensor 'mul_2:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_7:0' shape=() dtype=float32>, <tf.Tensor 'truediv_3:0' shape=() dtype=float32>, <tf.Operation 'Adam_11' type=NoOp>, (<tf.Variable 'lam_2:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=3 trainrate=0.5', <tf.Tensor 'mul_2:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_7:0' shape=() dtype=float32>, <tf.Tensor 'truediv_3:0' shape=() dtype=float32>, <tf.Operation 'Adam_12' type=NoOp>, ())\n",
      "('LISTA T=3 trainrate=0.1', <tf.Tensor 'mul_2:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_7:0' shape=() dtype=float32>, <tf.Tensor 'truediv_3:0' shape=() dtype=float32>, <tf.Operation 'Adam_13' type=NoOp>, ())\n",
      "('LISTA T=3 trainrate=0.01', <tf.Tensor 'mul_2:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_7:0' shape=() dtype=float32>, <tf.Tensor 'truediv_3:0' shape=() dtype=float32>, <tf.Operation 'Adam_14' type=NoOp>, ())\n",
      "('LISTA T=4', <tf.Tensor 'mul_3:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_9:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>, <tf.Operation 'Adam_15' type=NoOp>, (<tf.Variable 'lam_3:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=4 trainrate=0.5', <tf.Tensor 'mul_3:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_9:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>, <tf.Operation 'Adam_16' type=NoOp>, ())\n",
      "('LISTA T=4 trainrate=0.1', <tf.Tensor 'mul_3:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_9:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>, <tf.Operation 'Adam_17' type=NoOp>, ())\n",
      "('LISTA T=4 trainrate=0.01', <tf.Tensor 'mul_3:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_9:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>, <tf.Operation 'Adam_18' type=NoOp>, ())\n",
      "('LISTA T=5', <tf.Tensor 'mul_4:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_11:0' shape=() dtype=float32>, <tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Operation 'Adam_19' type=NoOp>, (<tf.Variable 'lam_4:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=5 trainrate=0.5', <tf.Tensor 'mul_4:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_11:0' shape=() dtype=float32>, <tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Operation 'Adam_20' type=NoOp>, ())\n",
      "('LISTA T=5 trainrate=0.1', <tf.Tensor 'mul_4:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_11:0' shape=() dtype=float32>, <tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Operation 'Adam_21' type=NoOp>, ())\n",
      "('LISTA T=5 trainrate=0.01', <tf.Tensor 'mul_4:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_11:0' shape=() dtype=float32>, <tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Operation 'Adam_22' type=NoOp>, ())\n",
      "('LISTA T=6', <tf.Tensor 'mul_5:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_13:0' shape=() dtype=float32>, <tf.Tensor 'truediv_6:0' shape=() dtype=float32>, <tf.Operation 'Adam_23' type=NoOp>, (<tf.Variable 'lam_5:0' shape=() dtype=float32_ref>,))\n",
      "('LISTA T=6 trainrate=0.5', <tf.Tensor 'mul_5:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_13:0' shape=() dtype=float32>, <tf.Tensor 'truediv_6:0' shape=() dtype=float32>, <tf.Operation 'Adam_24' type=NoOp>, ())\n",
      "('LISTA T=6 trainrate=0.1', <tf.Tensor 'mul_5:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_13:0' shape=() dtype=float32>, <tf.Tensor 'truediv_6:0' shape=() dtype=float32>, <tf.Operation 'Adam_25' type=NoOp>, ())\n",
      "('LISTA T=6 trainrate=0.01', <tf.Tensor 'mul_5:0' shape=(1600, ?) dtype=float32>, <tf.Tensor 'L2Loss_13:0' shape=() dtype=float32>, <tf.Tensor 'truediv_6:0' shape=() dtype=float32>, <tf.Operation 'Adam_26' type=NoOp>, ())\n"
     ]
    }
   ],
   "source": [
    "print(*training_stages , sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do training LISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trainable_vars(sess,filename,**kwargs):\n",
    "    \"\"\"save a .npz archive in `filename`  with\n",
    "    the current value of each variable in tf.trainable_variables()\n",
    "    plus any keyword numpy arrays.\n",
    "    \"\"\"\n",
    "    save={}\n",
    "    for v in tf.trainable_variables():\n",
    "        save[str(v.name)] = sess.run(v)\n",
    "    save.update(kwargs)\n",
    "    np.savez(filename,**save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainable_vars(sess,filename):\n",
    "    \"\"\"load a .npz archive and assign the value of each loaded\n",
    "    ndarray to the trainable variable whose name matches the\n",
    "    archive key.  Any elements in the archive that do not have\n",
    "    a corresponding trainable variable will be returned in a dict.\n",
    "    \"\"\"\n",
    "    other={}\n",
    "    try:\n",
    "        tv=dict([ (str(v.name),v) for v in tf.trainable_variables() ])\n",
    "        for k,d in np.load(filename).items():\n",
    "            if k in tv:\n",
    "                print('restoring ' + k)\n",
    "                sess.run(tf.assign( tv[k], d) )\n",
    "            else:\n",
    "                other[k] = d\n",
    "    except IOError:\n",
    "        pass\n",
    "    return other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naming convention:\n",
    "LISTA_SPARC_layers_rate_awgn_var.npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ivl, maxit, better_wait =10,1000000,3000\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('norms xval:{xval:.7f} yval:{yval:.7f}'.format(xval=la.norm(beta_val), yval=la.norm(y_val) ) )\n",
    "\n",
    "state = load_trainable_vars(sess,'LISTA_SPARC_6_R0p5_01.npz') # must load AFTER the initializer\n",
    "\n",
    "    # must use this same Session to perform all training\n",
    "    # if we start a new Session, things would replay and we'd be training with our validation set (no no)\n",
    "\n",
    "done=state.get('done',[])\n",
    "log=str(state.get('log',''))\n",
    "\n",
    "for name,xhat_,loss_,nmse_,train_,var_list in training_stages:\n",
    "    if name in done:\n",
    "        print('Already did ' + name + '. Skipping.')\n",
    "        continue\n",
    "    if len(var_list):\n",
    "        describe_var_list = 'extending ' + ','.join([v.name for v in var_list])\n",
    "    else:\n",
    "        describe_var_list = 'fine tuning all ' + ','.join([v.name for v in tf.trainable_variables() ])\n",
    "\n",
    "    print(name + ' ' + describe_var_list)\n",
    "    nmse_history=[]\n",
    "    for i in range(maxit+1):\n",
    "        if i%ivl == 0:\n",
    "            nmse = sess.run(nmse_,feed_dict={y_:y_val,x_:beta_val})\n",
    "            if np.isnan(nmse):\n",
    "                raise RuntimeError('nmse is NaN')\n",
    "            nmse_history = np.append(nmse_history,nmse)\n",
    "            nmse_dB = 10*np.log10(nmse)\n",
    "            nmsebest_dB = 10*np.log10(nmse_history.min())\n",
    "            sys.stdout.write('\\ri={i:<6d} nmse={nmse:.6f} dB (best={best:.6f})'.format(i=i,nmse=nmse_dB,best=nmsebest_dB))\n",
    "            sys.stdout.flush()\n",
    "            if i%(100*ivl) == 0:\n",
    "                print('')\n",
    "                age_of_best = len(nmse_history) - nmse_history.argmin()-1 # how long ago was the best nmse?\n",
    "                if age_of_best*ivl > better_wait:\n",
    "                    break # if it has not improved on the best answer for quite some time, then move along\n",
    "        #y,x = prob(sess)\n",
    "        y, x= sess.run( ( ygen_,xgen_ ) ) # generates different y,x for every iteration.\n",
    "        sess.run(train_,feed_dict={y_:y,x_:x} )\n",
    "    done = np.append(done,name)\n",
    "\n",
    "    log =  log+'\\n{name} nmse={nmse:.6f} dB in {i} iterations'.format(name=name,nmse=nmse_dB,i=i)\n",
    "\n",
    "    state['done'] = done\n",
    "    state['log'] = log\n",
    "    save_trainable_vars(sess,'LISTA_SPARC_6_R0p5_01.npz',**state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_test, beta_test2, n, colsby2_test = gen_msg(2,M, L, K, R, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.matmul(A, beta_test) + np.random.normal( size = (n,colsby2_test),scale= math.sqrt(awgn_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for name,xhat_,var_list in layers:\n",
    "    count = count + 1\n",
    "    if count != T+1:\n",
    "        continue\n",
    "    else:\n",
    "        out = sess.run(xhat_,feed_dict={y_:y_test,x_:beta_test})\n",
    "    #if count == 2:\n",
    "    #    out = sess.run(xhat_,feed_dict={y_:y_test,x_:beta_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest= \n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "estimated out \n",
      " [[ 0.3396565 ]\n",
      " [ 0.22745505]\n",
      " [-0.6091439 ]\n",
      " ...\n",
      " [-0.        ]\n",
      " [-0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('xtest= \\n',beta_test)\n",
    "print('estimated out \\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "-11.148475\n"
     ]
    }
   ],
   "source": [
    "print(beta_test.sum())\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "test_list = range(5)\n",
    "print(*test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "test2 = np.array(15)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "order0  = np.zeros(test2.shape + (3,), dtype=np.uint32)\n",
    "print(order0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "27581de6d80e5a5a3bac653d6793b7dc8a23f16ccb931cd5e47c2ad8a3cf817b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
